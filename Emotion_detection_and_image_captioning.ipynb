{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion_detection_and_image_captioning",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kashish-Agrawal/Emotion-detection-and-caption-generation/blob/main/Emotion_detection_and_image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgSqCv-h1PAJ"
      },
      "source": [
        "# Emotion Detection\n",
        "**Description:** Implement an emotion detection model using a simple CNN network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKEy5Qx01PAM"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDVfcTD623qA",
        "outputId": "83595253-4f09-41c3-de67-0413a4c87499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.64.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.10.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od"
      ],
      "metadata": {
        "id": "Fm6ndmue291r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Downloading dataset\n"
      ],
      "metadata": {
        "id": "4HfVAhfyj8CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/deadskull7/fer2013\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4PFPxVJ2_X7",
        "outputId": "ed5203c0-5c37-4d4b-f8f4-451c33799223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: abby14\n",
            "Your Kaggle Key: ··········\n",
            "Downloading fer2013.zip to ./fer2013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 96.6M/96.6M [00:00<00:00, 140MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "cxrGWE8q0Wk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "if os.path.exists('data'):\n",
        "    pass\n",
        "else:\n",
        "    os.mkdir('data')\n",
        "\n",
        "shutil.move('fer2013/fer2013.csv', 'data')\n",
        "os.rmdir('fer2013')"
      ],
      "metadata": {
        "id": "u2-69PxN3RET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all Necessary Libraries"
      ],
      "metadata": {
        "id": "Od4EBESpzLec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys,os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten  \n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.losses import categorical_crossentropy  \n",
        "from tensorflow.keras.optimizers import Adam  \n",
        "from tensorflow.keras.regularizers import l2 \n",
        "from tensorflow.keras import utils"
      ],
      "metadata": {
        "id": "udcgYcFj3uPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking whether the gpu is working fine\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(physical_devices)"
      ],
      "metadata": {
        "id": "qbgF78nA3zTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f4ed311-8cbc-46a9-9829-598a4a46b7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data pre-processing"
      ],
      "metadata": {
        "id": "Gn7rwX_Pkoy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reading file and extracting data\n",
        "\n",
        "emotion_data = pd.read_csv('data/fer2013.csv')\n",
        "\n",
        "# splitting the data into train and test sets\n",
        "\n",
        "X_train,train_y,X_test,test_y=[],[],[],[]  \n",
        "for index, row in emotion_data.iterrows():  \n",
        "    val=row['pixels'].split(\" \")  \n",
        "    if 'Training' in row['Usage']:\n",
        "      X_train.append(np.array(val,'float32'))  \n",
        "      train_y.append(row['emotion'])  \n",
        "    elif 'PublicTest' in row['Usage']:  \n",
        "      X_test.append(np.array(val,'float32'))  \n",
        "      test_y.append(row['emotion'])\n",
        "        \n",
        "num_features = 64  \n",
        "num_labels = 7\n",
        "\n",
        "emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
        "num_classes = len(emotion_labels)\n",
        "\n",
        "batch_size = 64  \n",
        "epochs = 124\n",
        "width, height = 48, 48 \n",
        "\n",
        "#Converting the image into an array of pixel numbers\n",
        "\n",
        "X_train = np.array(X_train,'float32')                                                         \n",
        "train_y = np.array(train_y,'float32')                                                                   \n",
        "X_test = np.array(X_test,'float32')                                                       \n",
        "test_y = np.array(test_y,'float32')                                                   \n",
        "train_y= utils.to_categorical(train_y, num_classes=num_labels)   # matching the pixel array to specified emotion label in train set              \n",
        "test_y= utils.to_categorical(test_y, num_classes=num_labels)     # matching the pixel array to specified emotion label in test set                    \n",
        "X_train -= np.mean(X_train, axis=0)                                                 \n",
        "X_train /= np.std(X_train, axis=0)                                                    \n",
        "X_test -= np.mean(X_test, axis=0)                                                                   \n",
        "X_test /= np.std(X_test, axis=0)                                                      \n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)                                              \n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)                                             \n",
        "\n",
        "print(emotion_data)                                                                                 "
      ],
      "metadata": {
        "id": "HELmDbWV367O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d738e0-06aa-4244-ec4e-e288e5db3599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       emotion                                             pixels        Usage\n",
            "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
            "1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
            "2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
            "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
            "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n",
            "...        ...                                                ...          ...\n",
            "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
            "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
            "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
            "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
            "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
            "\n",
            "[35887 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building Model"
      ],
      "metadata": {
        "id": "g1cvVLLolRKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples, num_classes = emotion_data.shape\n",
        "#Creating a CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='same', name='image_array', input_shape=(X_train.shape[1:])))\n",
        "model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='same'))\n",
        "model.add(BatchNormalization())  #using batch normalization to help in gradient descent\n",
        "model.add(Activation('relu')) \n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.3))               # adding dropout for regularization\n",
        "\n",
        "model.add(Conv2D(filters=96, kernel_size=(5, 5), padding='same')) #using same padding to retain the size of the image for easier processing\n",
        "model.add(Conv2D(filters=96, kernel_size=(5, 5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.3))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(5, 5), padding='same'))\n",
        "model.add(Conv2D(filters=128, kernel_size=(5, 5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.3))\n",
        "model.add(Conv2D(filters=256, kernel_size=(5, 5), padding='same'))\n",
        "model.add(Conv2D(filters=256, kernel_size=(5, 5), padding='same', activation='relu'))  \n",
        "model.add(BatchNormalization())  \n",
        "model.add(GlobalAveragePooling2D())  \n",
        "\n",
        "model.add(Flatten()) \n",
        "\n",
        "# Fully connected layer 1st layer\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(num_labels, activation='sigmoid'))  "
      ],
      "metadata": {
        "id": "4-3a9wN_39ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=categorical_crossentropy,  \n",
        "              optimizer=Adam(),  \n",
        "              metrics=['accuracy'])  \n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-f6PKWY84ICW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f203f7c9-37b7-4e55-fee3-338b8393163f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " image_array (Conv2D)        (None, 48, 48, 64)        1664      \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 48, 48, 64)        102464    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 48, 48, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 48, 48, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 24, 24, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 96)        153696    \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 24, 24, 96)        230496    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 24, 24, 96)       384       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 24, 24, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 96)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 12, 12, 96)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 12, 12, 128)       307328    \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 12, 12, 128)       409728    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 12, 12, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 6, 6, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 256)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 3591      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,869,703\n",
            "Trainable params: 3,867,079\n",
            "Non-trainable params: 2,624\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using early stopping to reduce overfitting\n",
        "cb_early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\",\n",
        "    min_delta=0.0005,\n",
        "    patience=11,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "#reducing learning rate when the gradient descent slows down\n",
        "cb_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_accuracy\",\n",
        "    factor=0.5,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    min_lr=1e-7,\n",
        ")"
      ],
      "metadata": {
        "id": "qD2qK29O4RXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Real time data augmentation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen=ImageDataGenerator(rescale=1.0/255.0, rotation_range=20,\n",
        "                                 height_shift_range=0.1,\n",
        "                                 width_shift_range=0.4,\n",
        "                                 shear_range=0.3,horizontal_flip=True,\n",
        "                                 zoom_range=0.2\n",
        "                                )\n",
        "validation_datagen=ImageDataGenerator(rescale=1.0/255.0) #scaling the input image\n",
        "train_datagen.fit(X_train)\n",
        "\n",
        "model.fit(train_datagen.flow(X_train, train_y, batch_size=batch_size),\n",
        "       \n",
        "         validation_data=validation_datagen.flow(X_test, test_y,\n",
        "         batch_size=batch_size),\n",
        "         steps_per_epoch=len(X_train) / batch_size, epochs=epochs, callbacks=[cb_early_stop,cb_reduce_lr])\n"
      ],
      "metadata": {
        "id": "l-Mr-2yR6yfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ef3c74-c3b6-429c-b3c1-dc1413a2dfc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/124\n",
            "448/448 [==============================] - 70s 127ms/step - loss: 1.9367 - accuracy: 0.2220 - val_loss: 1.8350 - val_accuracy: 0.2494 - lr: 0.0010\n",
            "Epoch 2/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.8422 - accuracy: 0.2470 - val_loss: 1.8600 - val_accuracy: 0.2249 - lr: 0.0010\n",
            "Epoch 3/124\n",
            "448/448 [==============================] - 54s 121ms/step - loss: 1.8053 - accuracy: 0.2534 - val_loss: 1.9423 - val_accuracy: 0.1666 - lr: 0.0010\n",
            "Epoch 4/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.7711 - accuracy: 0.2740 - val_loss: 2.0887 - val_accuracy: 0.1750 - lr: 0.0010\n",
            "Epoch 5/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.7260 - accuracy: 0.2925 - val_loss: 1.8420 - val_accuracy: 0.2733 - lr: 0.0010\n",
            "Epoch 6/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.6757 - accuracy: 0.3283 - val_loss: 1.8599 - val_accuracy: 0.2254 - lr: 0.0010\n",
            "Epoch 7/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.6376 - accuracy: 0.3510 - val_loss: 1.9329 - val_accuracy: 0.1942 - lr: 0.0010\n",
            "Epoch 8/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.5713 - accuracy: 0.3828 - val_loss: 1.8272 - val_accuracy: 0.3293 - lr: 0.0010\n",
            "Epoch 9/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.5134 - accuracy: 0.4123 - val_loss: 4.3287 - val_accuracy: 0.2006 - lr: 0.0010\n",
            "Epoch 10/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.4719 - accuracy: 0.4311 - val_loss: 1.4435 - val_accuracy: 0.4533 - lr: 0.0010\n",
            "Epoch 11/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.4320 - accuracy: 0.4437 - val_loss: 1.4894 - val_accuracy: 0.4246 - lr: 0.0010\n",
            "Epoch 12/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.4032 - accuracy: 0.4595 - val_loss: 1.4103 - val_accuracy: 0.4681 - lr: 0.0010\n",
            "Epoch 13/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.3756 - accuracy: 0.4710 - val_loss: 1.3776 - val_accuracy: 0.4681 - lr: 0.0010\n",
            "Epoch 14/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.3472 - accuracy: 0.4804 - val_loss: 1.4497 - val_accuracy: 0.4291 - lr: 0.0010\n",
            "Epoch 15/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.3298 - accuracy: 0.4881 - val_loss: 1.5259 - val_accuracy: 0.4232 - lr: 0.0010\n",
            "Epoch 16/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.3119 - accuracy: 0.4940 - val_loss: 1.2828 - val_accuracy: 0.5102 - lr: 0.0010\n",
            "Epoch 17/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.2944 - accuracy: 0.5086 - val_loss: 1.3578 - val_accuracy: 0.4851 - lr: 0.0010\n",
            "Epoch 18/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.2810 - accuracy: 0.5130 - val_loss: 1.2825 - val_accuracy: 0.5029 - lr: 0.0010\n",
            "Epoch 19/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.2688 - accuracy: 0.5175 - val_loss: 1.3440 - val_accuracy: 0.5079 - lr: 0.0010\n",
            "Epoch 20/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.2561 - accuracy: 0.5215 - val_loss: 1.4346 - val_accuracy: 0.4335 - lr: 0.0010\n",
            "Epoch 21/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.2464 - accuracy: 0.5228 - val_loss: 1.3173 - val_accuracy: 0.4834 - lr: 0.0010\n",
            "Epoch 22/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.2381 - accuracy: 0.5279 - val_loss: 1.2813 - val_accuracy: 0.5216 - lr: 0.0010\n",
            "Epoch 23/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.2260 - accuracy: 0.5335 - val_loss: 1.2579 - val_accuracy: 0.5280 - lr: 0.0010\n",
            "Epoch 24/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.2202 - accuracy: 0.5404 - val_loss: 1.3723 - val_accuracy: 0.4645 - lr: 0.0010\n",
            "Epoch 25/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.2049 - accuracy: 0.5454 - val_loss: 1.3044 - val_accuracy: 0.5026 - lr: 0.0010\n",
            "Epoch 26/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1990 - accuracy: 0.5454 - val_loss: 1.2170 - val_accuracy: 0.5428 - lr: 0.0010\n",
            "Epoch 27/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1875 - accuracy: 0.5508 - val_loss: 1.2895 - val_accuracy: 0.5088 - lr: 0.0010\n",
            "Epoch 28/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1849 - accuracy: 0.5531 - val_loss: 1.2684 - val_accuracy: 0.5202 - lr: 0.0010\n",
            "Epoch 29/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1802 - accuracy: 0.5527 - val_loss: 1.2731 - val_accuracy: 0.5118 - lr: 0.0010\n",
            "Epoch 30/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1672 - accuracy: 0.5560 - val_loss: 1.1606 - val_accuracy: 0.5662 - lr: 0.0010\n",
            "Epoch 31/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1615 - accuracy: 0.5606 - val_loss: 1.3273 - val_accuracy: 0.5077 - lr: 0.0010\n",
            "Epoch 32/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1566 - accuracy: 0.5641 - val_loss: 1.3700 - val_accuracy: 0.4645 - lr: 0.0010\n",
            "Epoch 33/124\n",
            "448/448 [==============================] - 54s 121ms/step - loss: 1.1580 - accuracy: 0.5620 - val_loss: 1.2327 - val_accuracy: 0.5305 - lr: 0.0010\n",
            "Epoch 34/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1499 - accuracy: 0.5656 - val_loss: 1.1915 - val_accuracy: 0.5514 - lr: 0.0010\n",
            "Epoch 35/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1407 - accuracy: 0.5678 - val_loss: 1.1444 - val_accuracy: 0.5603 - lr: 0.0010\n",
            "Epoch 36/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1384 - accuracy: 0.5704 - val_loss: 1.6588 - val_accuracy: 0.4174 - lr: 0.0010\n",
            "Epoch 37/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1320 - accuracy: 0.5710 - val_loss: 1.2656 - val_accuracy: 0.5141 - lr: 0.0010\n",
            "Epoch 38/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1287 - accuracy: 0.5709 - val_loss: 1.0694 - val_accuracy: 0.5985 - lr: 0.0010\n",
            "Epoch 39/124\n",
            "448/448 [==============================] - 54s 121ms/step - loss: 1.1161 - accuracy: 0.5784 - val_loss: 1.2282 - val_accuracy: 0.5461 - lr: 0.0010\n",
            "Epoch 40/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1162 - accuracy: 0.5793 - val_loss: 1.0911 - val_accuracy: 0.5926 - lr: 0.0010\n",
            "Epoch 41/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1192 - accuracy: 0.5776 - val_loss: 1.1992 - val_accuracy: 0.5545 - lr: 0.0010\n",
            "Epoch 42/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1132 - accuracy: 0.5806 - val_loss: 1.3865 - val_accuracy: 0.4868 - lr: 0.0010\n",
            "Epoch 43/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1022 - accuracy: 0.5845 - val_loss: 1.1964 - val_accuracy: 0.5553 - lr: 0.0010\n",
            "Epoch 44/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.1024 - accuracy: 0.5834 - val_loss: 1.2288 - val_accuracy: 0.5266 - lr: 0.0010\n",
            "Epoch 45/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0997 - accuracy: 0.5872 - val_loss: 1.4455 - val_accuracy: 0.4391 - lr: 0.0010\n",
            "Epoch 46/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0894 - accuracy: 0.5926 - val_loss: 1.3874 - val_accuracy: 0.4806 - lr: 0.0010\n",
            "Epoch 47/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0989 - accuracy: 0.5890 - val_loss: 1.1431 - val_accuracy: 0.5676 - lr: 0.0010\n",
            "Epoch 48/124\n",
            "449/448 [==============================] - ETA: 0s - loss: 1.0892 - accuracy: 0.5884\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0892 - accuracy: 0.5884 - val_loss: 1.1050 - val_accuracy: 0.5826 - lr: 0.0010\n",
            "Epoch 49/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0634 - accuracy: 0.5987 - val_loss: 1.0337 - val_accuracy: 0.6141 - lr: 5.0000e-04\n",
            "Epoch 50/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0565 - accuracy: 0.6000 - val_loss: 1.0845 - val_accuracy: 0.5882 - lr: 5.0000e-04\n",
            "Epoch 51/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0516 - accuracy: 0.6019 - val_loss: 1.0853 - val_accuracy: 0.5874 - lr: 5.0000e-04\n",
            "Epoch 52/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0506 - accuracy: 0.6037 - val_loss: 1.0805 - val_accuracy: 0.5971 - lr: 5.0000e-04\n",
            "Epoch 53/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0427 - accuracy: 0.6096 - val_loss: 1.0097 - val_accuracy: 0.6236 - lr: 5.0000e-04\n",
            "Epoch 54/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0459 - accuracy: 0.6066 - val_loss: 1.0179 - val_accuracy: 0.6160 - lr: 5.0000e-04\n",
            "Epoch 55/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0384 - accuracy: 0.6095 - val_loss: 1.0140 - val_accuracy: 0.6205 - lr: 5.0000e-04\n",
            "Epoch 56/124\n",
            "448/448 [==============================] - 54s 121ms/step - loss: 1.0378 - accuracy: 0.6115 - val_loss: 1.0051 - val_accuracy: 0.6317 - lr: 5.0000e-04\n",
            "Epoch 57/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0299 - accuracy: 0.6130 - val_loss: 1.0026 - val_accuracy: 0.6303 - lr: 5.0000e-04\n",
            "Epoch 58/124\n",
            "448/448 [==============================] - 54s 121ms/step - loss: 1.0341 - accuracy: 0.6101 - val_loss: 1.0709 - val_accuracy: 0.6071 - lr: 5.0000e-04\n",
            "Epoch 59/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0337 - accuracy: 0.6084 - val_loss: 1.0442 - val_accuracy: 0.6152 - lr: 5.0000e-04\n",
            "Epoch 60/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0350 - accuracy: 0.6101 - val_loss: 1.0523 - val_accuracy: 0.6013 - lr: 5.0000e-04\n",
            "Epoch 61/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0205 - accuracy: 0.6159 - val_loss: 1.0171 - val_accuracy: 0.6166 - lr: 5.0000e-04\n",
            "Epoch 62/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0261 - accuracy: 0.6118 - val_loss: 1.0487 - val_accuracy: 0.6236 - lr: 5.0000e-04\n",
            "Epoch 63/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0217 - accuracy: 0.6168 - val_loss: 1.0780 - val_accuracy: 0.6077 - lr: 5.0000e-04\n",
            "Epoch 64/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0192 - accuracy: 0.6172 - val_loss: 1.0830 - val_accuracy: 0.5963 - lr: 5.0000e-04\n",
            "Epoch 65/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0193 - accuracy: 0.6167 - val_loss: 1.0737 - val_accuracy: 0.5971 - lr: 5.0000e-04\n",
            "Epoch 66/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0204 - accuracy: 0.6154 - val_loss: 0.9754 - val_accuracy: 0.6467 - lr: 5.0000e-04\n",
            "Epoch 67/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0166 - accuracy: 0.6153 - val_loss: 0.9741 - val_accuracy: 0.6372 - lr: 5.0000e-04\n",
            "Epoch 68/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0114 - accuracy: 0.6206 - val_loss: 1.0051 - val_accuracy: 0.6272 - lr: 5.0000e-04\n",
            "Epoch 69/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0117 - accuracy: 0.6183 - val_loss: 1.0276 - val_accuracy: 0.6135 - lr: 5.0000e-04\n",
            "Epoch 70/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0093 - accuracy: 0.6183 - val_loss: 1.0448 - val_accuracy: 0.6233 - lr: 5.0000e-04\n",
            "Epoch 71/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0077 - accuracy: 0.6212 - val_loss: 0.9884 - val_accuracy: 0.6342 - lr: 5.0000e-04\n",
            "Epoch 72/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0074 - accuracy: 0.6197 - val_loss: 0.9762 - val_accuracy: 0.6473 - lr: 5.0000e-04\n",
            "Epoch 73/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0058 - accuracy: 0.6186 - val_loss: 1.0583 - val_accuracy: 0.6088 - lr: 5.0000e-04\n",
            "Epoch 74/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 0.9993 - accuracy: 0.6247 - val_loss: 1.1095 - val_accuracy: 0.5915 - lr: 5.0000e-04\n",
            "Epoch 75/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 1.0069 - accuracy: 0.6205 - val_loss: 1.0224 - val_accuracy: 0.6300 - lr: 5.0000e-04\n",
            "Epoch 76/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 0.9991 - accuracy: 0.6226 - val_loss: 1.0153 - val_accuracy: 0.6180 - lr: 5.0000e-04\n",
            "Epoch 77/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 0.9976 - accuracy: 0.6248 - val_loss: 0.9656 - val_accuracy: 0.6439 - lr: 5.0000e-04\n",
            "Epoch 78/124\n",
            "448/448 [==============================] - 54s 121ms/step - loss: 0.9970 - accuracy: 0.6237 - val_loss: 1.0807 - val_accuracy: 0.6030 - lr: 5.0000e-04\n",
            "Epoch 79/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 0.9984 - accuracy: 0.6245 - val_loss: 1.0112 - val_accuracy: 0.6264 - lr: 5.0000e-04\n",
            "Epoch 80/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 0.9958 - accuracy: 0.6262 - val_loss: 1.0385 - val_accuracy: 0.6227 - lr: 5.0000e-04\n",
            "Epoch 81/124\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 0.9943 - accuracy: 0.6241 - val_loss: 1.0948 - val_accuracy: 0.5918 - lr: 5.0000e-04\n",
            "Epoch 82/124\n",
            "449/448 [==============================] - ETA: 0s - loss: 0.9906 - accuracy: 0.6279\n",
            "Epoch 82: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 0.9906 - accuracy: 0.6279 - val_loss: 0.9971 - val_accuracy: 0.6222 - lr: 5.0000e-04\n",
            "Epoch 83/124\n",
            "449/448 [==============================] - ETA: 0s - loss: 0.9819 - accuracy: 0.6317Restoring model weights from the end of the best epoch: 72.\n",
            "448/448 [==============================] - 54s 120ms/step - loss: 0.9819 - accuracy: 0.6317 - val_loss: 0.9844 - val_accuracy: 0.6378 - lr: 2.5000e-04\n",
            "Epoch 83: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5551817350>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "         optimizer=Adam(),\n",
        "         metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "W9ARTyfW4hVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the model on new images\n",
        "score = model.predict(X_test)\n",
        "\n",
        "pred_y = [np.argmax(item) for item in score]\n",
        "true_y = [np.argmax(item) for item in test_y]\n",
        "\n",
        "accuracy = [(x==y) for x,y in zip(pred_y,true_y)]\n",
        "print(\"accuracy on testset: \" , np.mean(accuracy))"
      ],
      "metadata": {
        "id": "51CCdeWX4i0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6035e437-1bd1-4442-edf3-c84c9f81ef1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on testset:  0.1390359431596545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Captioning\n",
        "**description**: using a pre trained CNN (EfficientNet) and transformer for captioning images of flickr8k dataset"
      ],
      "metadata": {
        "id": "5WdJE5EZmH9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing libraries"
      ],
      "metadata": {
        "id": "_8fQ9P59m4CA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuTEVxo01PAM"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import efficientnet\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "\n",
        "seed = 111\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrbAvLpF1PAN"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "We will be using the Flickr8K dataset. This dataset comprises over\n",
        "8,000 images, that are each paired with five different captions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaEhIUPS1PAO"
      },
      "source": [
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip -qq Flickr8k_Dataset.zip\n",
        "!unzip -qq Flickr8k_text.zip\n",
        "!rm Flickr8k_Dataset.zip Flickr8k_text.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRXb7rbP1PAO"
      },
      "source": [
        "\n",
        "# Path to the images\n",
        "IMAGES_PATH = \"Flicker8k_Dataset\"\n",
        "\n",
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 20\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 512\n",
        "\n",
        "# Number of self-attention heads\n",
        "NUM_HEADS = 2\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "FF_DIM = 512\n",
        "\n",
        "# Other training parameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 75\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMpPTnsE1PAP"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YCLzwoP1PAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a91d47de-aa20-4bb6-ec0a-13d725cf6ea8"
      },
      "source": [
        "\n",
        "def load_captions_data(filename):\n",
        "    \"\"\"Loads captions (text) data and maps them to corresponding images.\n",
        "\n",
        "    Args:\n",
        "        filename: Path to the text file containing caption data.\n",
        "\n",
        "    Returns:\n",
        "        caption_mapping: Dictionary mapping image names and the corresponding captions\n",
        "        text_data: List containing all the available captions\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename) as caption_file:\n",
        "        caption_data = caption_file.readlines()\n",
        "        caption_mapping = {}\n",
        "        text_data = []\n",
        "\n",
        "        for line in caption_data:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            # Image name and captions are separated using a tab\n",
        "            img_name, caption = line.split(\"\\t\")\n",
        "            # Each image is repeated five times for the five different captions. Each\n",
        "            # image name has a prefix `#(caption_number)`\n",
        "            img_name = img_name.split(\"#\")[0]\n",
        "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
        "\n",
        "            if img_name.endswith(\"jpg\"):\n",
        "                # We will add a start and an end token to each caption\n",
        "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "                text_data.append(caption)\n",
        "\n",
        "                if img_name in caption_mapping:\n",
        "                    caption_mapping[img_name].append(caption)\n",
        "                else:\n",
        "                    caption_mapping[img_name] = [caption]\n",
        "\n",
        "        return caption_mapping, text_data\n",
        "\n",
        "\n",
        "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
        "    \"\"\"Split the captioning dataset into train and validation sets.\n",
        "\n",
        "    Args:\n",
        "        caption_data (dict): Dictionary containing the mapped caption data\n",
        "        train_size (float): Fraction of all the full dataset to use as training data\n",
        "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
        "\n",
        "    Returns:\n",
        "        Traning and validation datasets as two separated dicts\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get the list of all image names\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    # 2. Shuffle if necessary\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    # 3. Split into training and validation sets\n",
        "    train_size = int(len(caption_data) * train_size)\n",
        "\n",
        "    training_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
        "    }\n",
        "    validation_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
        "    }\n",
        "\n",
        "    # 4. Return the splits\n",
        "    return training_data, validation_data\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, valid_data = train_val_split(captions_mapping)\n",
        "print(\"Number of training samples: \", len(train_data))\n",
        "print(\"Number of validation samples: \", len(valid_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  6472\n",
            "Number of validation samples:  1619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sGPcSr91PAT"
      },
      "source": [
        "## Vectorizing the text data\n",
        "\n",
        "We'll use the `TextVectorization` layer to vectorize the text data,\n",
        "that is to say, to turn the\n",
        "original strings into integer sequences where each integer represents the index of\n",
        "a word in a vocabulary. We will use a custom string standardization scheme\n",
        "(strip punctuation characters except `<` and `>`) and the default\n",
        "splitting scheme (split on whitespace)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm5XQJJS1PAU"
      },
      "source": [
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "vectorization.adapt(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qf7Au_c1PAV"
      },
      "source": [
        "## Building a `tf.data.Dataset` pipeline for training\n",
        "\n",
        "We will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\n",
        "The pipeline consists of two steps:\n",
        "\n",
        "1. Read the image from the disk\n",
        "2. Tokenize all the five captions corresponding to the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L7no0Nl1PAV"
      },
      "source": [
        "\n",
        "def read_image(img_path, size=IMAGE_SIZE):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def make_dataset(images, captions):\n",
        "    img_dataset = tf.data.Dataset.from_tensor_slices(images).map(\n",
        "        read_image, num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    cap_dataset = tf.data.Dataset.from_tensor_slices(captions).map(\n",
        "        vectorization, num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    dataset = tf.data.Dataset.zip((img_dataset, cap_dataset))\n",
        "    dataset = dataset.batch(BATCH_SIZE).shuffle(256).prefetch(AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Pass the list of images and the list of corresponding captions\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
        "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a173MVtm1PAW"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Our image captioning architecture consists of three models:\n",
        "\n",
        "1. A CNN: used to extract the image features\n",
        "2. A TransformerEncoder: The extracted image features are then passed to a Transformer\n",
        "                    based encoder that generates a new representation of the inputs\n",
        "3. A TransformerDecoder: This model takes the encoder output and the text data\n",
        "                    (sequences) as inputs and tries to learn to generate the caption."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvuQM28A1PAW",
        "outputId": "297aa77f-59e9-426b-e246-1833f660b101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "def get_cnn_model():\n",
        "    base_model = efficientnet.EfficientNetB0(\n",
        "        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n",
        "    )\n",
        "    # We freeze our feature extractor\n",
        "    base_model.trainable = False\n",
        "    base_model_out = base_model.output\n",
        "    base_model_out = layers.Reshape((-1, 1280))(base_model_out)\n",
        "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
        "    return cnn_model\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = layers.Dense(embed_dim, activation=\"relu\")\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        inputs = self.dense_proj(inputs)\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=None\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        return proj_input\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "        self.embedding = PositionalEmbedding(\n",
        "            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n",
        "        )\n",
        "        self.out = layers.Dense(VOCAB_SIZE)\n",
        "        self.dropout_1 = layers.Dropout(0.1)\n",
        "        self.dropout_2 = layers.Dropout(0.5)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
        "        inputs = self.embedding(inputs)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        inputs = self.dropout_1(inputs, training=training)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=combined_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        proj_out = self.layernorm_3(out_2 + proj_output)\n",
        "        proj_out = self.dropout_2(proj_out, training=training)\n",
        "\n",
        "        preds = self.out(proj_out)\n",
        "        return preds\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class ImageCaptioningModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, cnn_model, encoder, decoder, num_captions_per_image=5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "        self.num_captions_per_image = num_captions_per_image\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    def _compute_loss_and_acc(self, batch_data, training=True):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # 3. Pass image embeddings to encoder\n",
        "                encoder_out = self.encoder(img_embed, training=training)\n",
        "\n",
        "                batch_seq_inp = batch_seq[:, i, :-1]\n",
        "                batch_seq_true = batch_seq[:, i, 1:]\n",
        "\n",
        "                # 4. Compute the mask for the input sequence\n",
        "                mask = tf.math.not_equal(batch_seq_inp, 0)\n",
        "\n",
        "                # 5. Pass the encoder outputs, sequence inputs along with\n",
        "                # mask to the decoder\n",
        "                batch_seq_pred = self.decoder(\n",
        "                    batch_seq_inp, encoder_out, training=training, mask=mask\n",
        "                )\n",
        "\n",
        "                # 6. Calculate loss and accuracy\n",
        "                loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "                acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "\n",
        "                # 7. Update the batch loss and batch accuracy\n",
        "                batch_loss += loss\n",
        "                batch_acc += acc\n",
        "\n",
        "            # 8. Get the list of all the trainable weights\n",
        "            train_vars = (\n",
        "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "            )\n",
        "\n",
        "            # 9. Get the gradients\n",
        "            grads = tape.gradient(loss, train_vars)\n",
        "\n",
        "            # 10. Update the trainable weights\n",
        "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "\n",
        "        return batch_loss, batch_acc / float(self.num_captions_per_image)\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        loss, acc = self._compute_loss_and_acc(batch_data)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    def test_step(self, batch_data):\n",
        "        loss, acc = self._compute_loss_and_acc(batch_data, training=False)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We need to list our metrics here so the `reset_states()` can be\n",
        "        # called automatically.\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "\n",
        "cnn_model = get_cnn_model()\n",
        "encoder = TransformerEncoderBlock(\n",
        "    embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=NUM_HEADS\n",
        ")\n",
        "decoder = TransformerDecoderBlock(\n",
        "    embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=NUM_HEADS\n",
        ")\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model, encoder=encoder, decoder=decoder\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "16719872/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vK87n3o1PAZ"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M9U9SoqEmpLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyCTP-sn1PAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2010f57a-5b2b-4de8-f111-f06f014808ea"
      },
      "source": [
        "# Define the loss function\n",
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "# EarlyStopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Compile the model\n",
        "caption_model.compile(optimizer=keras.optimizers.Adam(), loss=cross_entropy)\n",
        "\n",
        "\n",
        "\n",
        "# Fit the model\n",
        "caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "102/102 [==============================] - 278s 2s/step - loss: 17.8488 - acc: 0.3338 - val_loss: 15.0015 - val_acc: 0.4237\n",
            "Epoch 2/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 14.5668 - acc: 0.4245 - val_loss: 12.9955 - val_acc: 0.4547\n",
            "Epoch 3/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 13.4003 - acc: 0.4512 - val_loss: 11.7420 - val_acc: 0.4803\n",
            "Epoch 4/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 12.5769 - acc: 0.4681 - val_loss: 10.8269 - val_acc: 0.5018\n",
            "Epoch 5/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 11.8947 - acc: 0.4837 - val_loss: 10.1183 - val_acc: 0.5212\n",
            "Epoch 6/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 11.3275 - acc: 0.4966 - val_loss: 9.4861 - val_acc: 0.5409\n",
            "Epoch 7/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 10.8687 - acc: 0.5033 - val_loss: 8.9868 - val_acc: 0.5556\n",
            "Epoch 8/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 10.4192 - acc: 0.5202 - val_loss: 8.5288 - val_acc: 0.5737\n",
            "Epoch 9/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 10.0623 - acc: 0.5308 - val_loss: 8.3113 - val_acc: 0.5782\n",
            "Epoch 10/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 9.7273 - acc: 0.5354 - val_loss: 7.7654 - val_acc: 0.5988\n",
            "Epoch 11/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 9.3826 - acc: 0.5502 - val_loss: 7.4096 - val_acc: 0.6133\n",
            "Epoch 12/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 9.1273 - acc: 0.5617 - val_loss: 7.1455 - val_acc: 0.6251\n",
            "Epoch 13/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 8.9455 - acc: 0.5685 - val_loss: 6.9427 - val_acc: 0.6321\n",
            "Epoch 14/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 8.7356 - acc: 0.5749 - val_loss: 6.7486 - val_acc: 0.6363\n",
            "Epoch 15/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 8.5998 - acc: 0.5793 - val_loss: 6.7197 - val_acc: 0.6377\n",
            "Epoch 16/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 8.3473 - acc: 0.5858 - val_loss: 6.2239 - val_acc: 0.6601\n",
            "Epoch 17/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 8.1638 - acc: 0.5953 - val_loss: 6.0921 - val_acc: 0.6661\n",
            "Epoch 18/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 7.9797 - acc: 0.6011 - val_loss: 5.8429 - val_acc: 0.6775\n",
            "Epoch 19/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 7.8228 - acc: 0.6080 - val_loss: 5.7997 - val_acc: 0.6747\n",
            "Epoch 20/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 7.8679 - acc: 0.5944 - val_loss: 5.6319 - val_acc: 0.6838\n",
            "Epoch 21/75\n",
            "102/102 [==============================] - 247s 2s/step - loss: 7.6276 - acc: 0.6097 - val_loss: 5.4147 - val_acc: 0.6946\n",
            "Epoch 22/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 7.4220 - acc: 0.6206 - val_loss: 5.1680 - val_acc: 0.7065\n",
            "Epoch 23/75\n",
            "102/102 [==============================] - 249s 2s/step - loss: 7.2722 - acc: 0.6267 - val_loss: 5.0836 - val_acc: 0.7109\n",
            "Epoch 24/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 7.2046 - acc: 0.6328 - val_loss: 5.1342 - val_acc: 0.7083\n",
            "Epoch 25/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 7.2835 - acc: 0.6264 - val_loss: 4.9981 - val_acc: 0.7138\n",
            "Epoch 26/75\n",
            "102/102 [==============================] - 248s 2s/step - loss: 7.0931 - acc: 0.6370 - val_loss: 4.8729 - val_acc: 0.7187\n",
            "Epoch 27/75\n",
            " 35/102 [=========>....................] - ETA: 1:50 - loss: 6.8424 - acc: 0.6376"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchtext.data.metrics.bleu_score( batch_seq_pred, batch_seq_true, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])"
      ],
      "metadata": {
        "id": "UeYRLMoXyA8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmelnjs21PAb"
      },
      "source": [
        "### Check sample predictions for emotion detection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "\n",
        "sample_number =  225 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "\n",
        "image = np.mat(emotion_data.pixels[sample_number]).reshape(48,48)\n",
        "plt.imshow(image, interpolation='nearest')\n",
        "plt.show()\n",
        "\n",
        "detected_emotion = emotion_labels[emotion_data.emotion[sample_number]]\n",
        "print(detected_emotion)"
      ],
      "metadata": {
        "id": "izaiYHjU4seu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check sample predictions for image captioning"
      ],
      "metadata": {
        "id": "qL5qRPpmnNgN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mvdCAxG1PAb"
      },
      "source": [
        "vocab = vectorization.get_vocabulary()\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
        "valid_images = list(valid_data.keys())\n",
        "\n",
        "\n",
        "def generate_caption():\n",
        "    # Select a random image from the validation dataset\n",
        "    sample_img = np.random.choice(valid_images)\n",
        "\n",
        "    # Read the image from the disk\n",
        "    sample_img = read_image(sample_img)\n",
        "    img = sample_img.numpy().astype(np.uint8)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    # Pass the image to the CNN\n",
        "    img = tf.expand_dims(sample_img, 0)\n",
        "    img = caption_model.cnn_model(img)\n",
        "\n",
        "    # Pass the image features to the Transformer encoder\n",
        "    encoded_img = caption_model.encoder(img, training=False)\n",
        "\n",
        "    # Generate the caption using the Transformer decoder\n",
        "    decoded_caption = \"<start> \"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
        "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "        predictions = caption_model.decoder(\n",
        "            tokenized_caption, encoded_img, training=False, mask=mask\n",
        "        )\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = index_lookup[sampled_token_index]\n",
        "        if sampled_token == \" <end>\":\n",
        "            break\n",
        "        decoded_caption += \" \" + sampled_token\n",
        "\n",
        "    print(\"PREDICTED CAPTION:\", end=\" \")\n",
        "    print(decoded_caption.replace(\"<start> \", \"\").replace(\" <end>\", \"\").strip())\n",
        "\n",
        "\n",
        "# Check predictions for a few samples\n",
        "generate_caption()\n",
        "generate_caption()\n",
        "generate_caption()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NhKc8bUOC8qW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}